{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Transform all the text information into utf8 format\n",
    "2. Make dictionaries to map concept, ordo_id, and corresponding synonyms together\n",
    "3. Prepare the relationship of the snomed terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/liangyuzhao/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import json\n",
    "from queue import LifoQueue\n",
    "from fuzzywuzzy import fuzz\n",
    "import os\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "import requests\n",
    "import getpass\n",
    "from bs4 import BeautifulSoup\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0-I Get the ORDO information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total number of ORDO disease terms: 3859\n",
      "The total number of SNOMED disease terms: 115396\n"
     ]
    }
   ],
   "source": [
    "# ## Read vocab csv file into dataframe\n",
    "ORDO = pd.read_csv(\"Vocabularies/ORDO.csv\", sep = ',')\n",
    "\n",
    "SNOMED = pd.read_csv(\"Vocabularies/SnomedCT_USEditionRF2_PRODUCTION_20190901T120000Z/Full/Terminology/sct2_Description_Full-en_US1000124_20190901.txt\",\n",
    "                    sep = '\\t')\n",
    "## Rename ORDO headers by stripping URLs\n",
    "\n",
    "ORDOcols = list(ORDO.columns)\n",
    "\n",
    "for i, colname in enumerate(ORDOcols):\n",
    "    # Check for URL\n",
    "    if \"http\" in colname:\n",
    "        # Reverse split (get only the substring behind the last / in the URL)\n",
    "        newcolname = colname.rsplit('/', 1)[1]\n",
    "        # Rename the column in ORDO dataframe\n",
    "        ORDO = ORDO.rename(columns = {colname: newcolname})\n",
    "## Extract ORDO disease terms (might expand to other classes later)\n",
    "\"\"\"\n",
    "'Parents' column has URL for 2nd level classes\n",
    "Clinical entity subclasses:\n",
    "    Biological anomaly: http://www.orpha.net/ORDO/Orphanet_377790\n",
    "    Clinical subtype: http://www.orpha.net/ORDO/Orphanet_377796\n",
    "    Clinical syndrome: http://www.orpha.net/ORDO/Orphanet_377792\n",
    "    Disease: http://www.orpha.net/ORDO/Orphanet_377788\n",
    "    Etiological subtype: http://www.orpha.net/ORDO/Orphanet_377795\n",
    "    Group of disorders: http://www.orpha.net/ORDO/Orphanet_377794\n",
    "    Histopathological subtype: http://www.orpha.net/ORDO/Orphanet_377797\n",
    "    Malformation syndrome: http://www.orpha.net/ORDO/Orphanet_377789\n",
    "    Morphological anomaly: http://www.orpha.net/ORDO/Orphanet_377791\n",
    "    Particular clinical situation in disease/syndrome: http://www.orpha.net/ORDO/Orphanet_377793   \n",
    "\"\"\"\n",
    "ORDOdiseases = ORDO[ORDO[\"Parents\"] == \"http://www.orpha.net/ORDO/Orphanet_377788\"]\n",
    "print(\"The total number of ORDO disease terms: \" + str(len(ORDOdiseases[\"Class ID\"].unique())))\n",
    "## Total number of SNOMED disease terms\n",
    "SNOMEDdiseases = SNOMED[SNOMED['term'].str.contains(\"\\(disorder\\)\") == True]\n",
    "\n",
    "unique_SNOMED = SNOMEDdiseases.drop_duplicates(subset = 'term')\n",
    "\n",
    "print(\"The total number of SNOMED disease terms: \" + str(len(unique_SNOMED)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Write all ORDO disease terms and SNOMED disease terms to files\n",
    "basepath = \"./Vocabularies/\"\n",
    "ORDOdiseases.to_csv(basepath + \"ORDO_diseases.csv\")\n",
    "unique_SNOMED.to_csv(basepath + \"SNOMED_diseases.csv\")  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I. Find Exact Match & UMLS Matches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A. Find the Exact Match between ORDO and SNOMED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of ORDO terms matched to SNOMED by string matching: 1526\n",
      "Number of ORDO terms not matched to SNOMED by string matching: 2333\n"
     ]
    }
   ],
   "source": [
    "## Number of ORDO disease terms with/without SNOMED similarity\n",
    "SNOMEDterms = list(unique_SNOMED['term'])\n",
    "SNOMEDlower = [(term.replace(\" (disorder)\", \"\")).lower() for term in SNOMEDterms]\n",
    "\n",
    "columns = ['ORDO_ID', 'ORDO_term', 'ORDO_syn', 'ORDO_definition', 'ORDO_mappings', \n",
    "          'SNOMED_ID', 'SNOMED_term']\n",
    "st_dict = dict((key,[]) for key in columns)\n",
    "dt_dict = dict((key,[]) for key in columns)\n",
    "\n",
    "for i, term in enumerate(ORDOdiseases[\"Preferred Label\"]):\n",
    "    if term.lower() in SNOMEDlower:\n",
    "        # Get SNOMED ID for term and add to similar terms dictionary\n",
    "        snomedIdx = SNOMEDlower.index(term.lower())\n",
    "        snomedID = unique_SNOMED.iloc[snomedIdx]['conceptId']\n",
    "        snomedTerm = unique_SNOMED.iloc[snomedIdx]['term']\n",
    "        st_dict['SNOMED_ID'] += [snomedID]\n",
    "        st_dict['SNOMED_term'] += [snomedTerm]\n",
    "        \n",
    "        # Add to ORDO info to similar terms dictionary\n",
    "        ORDO_ID, ORDO_term = ORDOdiseases.iloc[i]['Class ID'], ORDOdiseases.iloc[i]['Preferred Label']\n",
    "        ORDO_syn, ORDO_def, ORDO_mappings = ORDOdiseases.iloc[i]['Synonyms'], ORDOdiseases.iloc[i]['Definitions'], ORDOdiseases.iloc[i]['oboInOwl#hasDbXref']\n",
    "        st_dict['ORDO_ID'] += [ORDO_ID]\n",
    "        st_dict['ORDO_term'] += [ORDO_term] \n",
    "        st_dict['ORDO_syn'] += [ORDO_syn]\n",
    "        st_dict['ORDO_definition'] += [ORDO_def]\n",
    "        st_dict['ORDO_mappings'] += [ORDO_mappings]\n",
    "    \n",
    "    else:\n",
    "        # Add to ORDO info to dissimilar terms dictionary\n",
    "        ORDO_ID, ORDO_term = ORDOdiseases.iloc[i]['Class ID'], ORDOdiseases.iloc[i]['Preferred Label']\n",
    "        ORDO_syn, ORDO_def, ORDO_mappings = ORDOdiseases.iloc[i]['Synonyms'], ORDOdiseases.iloc[i]['Definitions'], ORDOdiseases.iloc[i]['oboInOwl#hasDbXref']\n",
    "        dt_dict['ORDO_ID'] += [ORDO_ID]\n",
    "        dt_dict['ORDO_term'] += [ORDO_term] \n",
    "        dt_dict['ORDO_syn'] += [ORDO_syn]\n",
    "        dt_dict['ORDO_definition'] += [ORDO_def]\n",
    "        dt_dict['ORDO_mappings'] += [ORDO_mappings]\n",
    "        dt_dict['SNOMED_ID'] += [\"\"]\n",
    "        dt_dict['SNOMED_term'] += [\"\"]\n",
    "        \n",
    "# Convert to dataframes\n",
    "similar_terms = pd.DataFrame.from_dict(st_dict)\n",
    "dissimilar_terms = pd.DataFrame.from_dict(dt_dict)\n",
    "\n",
    "print(\"Number of ORDO terms matched to SNOMED by string matching: \" + str(len(similar_terms)))\n",
    "print(\"Number of ORDO terms not matched to SNOMED by string matching: \" + str(len(dissimilar_terms)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "ordo_id = []\n",
    "concept = []\n",
    "score = []\n",
    "snomedid = []\n",
    "snomedterm = []\n",
    "for i in range(len(similar_terms.ORDO_ID.tolist())):\n",
    "    ordo = similar_terms.ORDO_ID.tolist()[i]\n",
    "    ordo = ordo.split('/')[-1].split('_')[1]\n",
    "    ordo_id.append(ordo)\n",
    "    score.append(1000)\n",
    "    concept.append(similar_terms.ORDO_term.tolist()[i])\n",
    "    snomedid.append(similar_terms.SNOMED_ID.tolist()[i])\n",
    "    snomedterm.append(similar_terms.SNOMED_term.tolist()[i])\n",
    "df_Mapped_ORDO_SNOMED_Direct = pd.DataFrame({'ORDO-ID': ordo_id, \n",
    "                                      'Concept':concept,\n",
    "                                     'Mapped_Term': concept,\n",
    "                                     'SNOMED_ID': snomedid,\n",
    "                                     'SNOMED_Term': snomedterm,\n",
    "                                     'Score': score})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Mapped_ORDO_SNOMED_Direct.to_csv(\"Results/Mapped_Results/Mapped_ORDO_SNOMED_Direct.txt\",sep=\"\\t\")\n",
    "\n",
    "dissimilar_terms.to_csv('ORDO_info/ORDO-SNOMED-not-matched.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B. Find the ORDO-SNOMED Map through UMLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please enter your username: ilyia1997\n",
      "Please enter your password: ········\n",
      "1137 runs left\n",
      "1037 runs left\n",
      "937 runs left\n",
      "837 runs left\n",
      "737 runs left\n",
      "637 runs left\n",
      "537 runs left\n",
      "437 runs left\n",
      "337 runs left\n",
      "237 runs left\n",
      "137 runs left\n",
      "37 runs left\n",
      "Number of ORDO terms mapped to SNOMED through UMLS: 909\n",
      "Number of ORDO terms not mapped to UMLS or to SNOMED through UMLS: 1424\n"
     ]
    }
   ],
   "source": [
    "## For ORDO terms unmatched to SNOMED: Which and how many have UMLS mappings to SNOMED?\n",
    "### NOTE: This will take a while to run since regex is slow, and each API request takes a few seconds ### \n",
    "\n",
    "UMLS_mapped = dissimilar_terms[dissimilar_terms['ORDO_mappings'].str.contains(\"UMLS\", na = False) == True]\n",
    "UMLS_unmapped = dissimilar_terms[dissimilar_terms['ORDO_mappings'].str.contains(\"UMLS\", na = False) == False]\n",
    "\n",
    "CUIpattern = re.compile(\"C\\d{7}\")\n",
    "\n",
    "# UMLS API requires getting TGT every 8 hours: https://documentation.uts.nlm.nih.gov/rest/authentication.html\n",
    "# Or just get every run (below)\n",
    "headers = {\"Content-Type\": \"application/x-www-form-urlencoded\"}\n",
    "\n",
    "user = input(\"Please enter your username: \")\n",
    "pw = getpass.getpass(\"Please enter your password: \")\n",
    "params = {\"username\" : user, \n",
    "         \"password\" : pw}\n",
    "\n",
    "TGT_URL = \"https://utslogin.nlm.nih.gov/cas/v1/tickets\"\n",
    "\n",
    "response = (requests.post(TGT_URL, headers = headers, params = params)).text\n",
    "ticketgetter = BeautifulSoup(response)\n",
    "TGT = ticketgetter.form['action']\n",
    "\n",
    "# For service ticket request\n",
    "headers_ST = {\"Content-Type\": \"application/x-www-form-urlencoded\"}\n",
    "params = {\"service\": \"http://umlsks.nlm.nih.gov\"}\n",
    "\n",
    "# Check CUIs for each mapping in UMLS_mapped\n",
    "columns = ['ORDO_ID', 'ORDO_term', 'ORDO_syn', 'ORDO_definition', 'ORDO_mappings', \n",
    "          'SNOMED_ID', 'SNOMED_term']\n",
    "mapped_dict = dict((key,[]) for key in columns)\n",
    "unmapped_dict = dict((key,[]) for key in columns)\n",
    "counter = 0\n",
    "for i, mapping in enumerate(UMLS_mapped['ORDO_mappings']):\n",
    "    CUI_list = re.findall(CUIpattern, str(mapping))\n",
    "    CUI = CUI_list[0]\n",
    "    # Request UMLS concept info from API         \n",
    "    # Every request requires a service ticket (use TGT to get service ticket)\n",
    "    ST = requests.post(TGT, headers = headers_ST, params = params)\n",
    "    URL = \"https://uts-ws.nlm.nih.gov/rest/content/current/CUI/{}/atoms?sabs=SNOMEDCT_US&ticket={}\".format(CUI, ST.text)\n",
    "    response = requests.get(URL)    \n",
    "\n",
    "    # Check if SNOMED-CT is mapped; get SNOMED ID\n",
    "    if response.text[0] == \"{\":     # \"{\" indicates start of response with AUIs; \"<\" indicates page not found (no AUIs)\n",
    "        atom_dict = json.loads(response.text)\n",
    "        atom_list = atom_dict['result']\n",
    "        # Atom list contains 2 SNOMED atoms, but both are same concept/code. Can use second one which is Fully-specified name\n",
    "        atom = atom_list[0]\n",
    "        SNOMED_term = atom['name']\n",
    "        SNOMED_code = atom['code'].rsplit('/', 1)[1]\n",
    "        # Add term info to mapped_dict\n",
    "        mapped_dict['ORDO_ID'] += [UMLS_mapped.iloc[i]['ORDO_ID']]\n",
    "        mapped_dict['ORDO_term'] += [UMLS_mapped.iloc[i]['ORDO_term']] \n",
    "        mapped_dict['ORDO_syn'] += [UMLS_mapped.iloc[i]['ORDO_syn']]\n",
    "        mapped_dict['ORDO_definition'] += [UMLS_mapped.iloc[i]['ORDO_definition']]\n",
    "        mapped_dict['ORDO_mappings'] += [UMLS_mapped.iloc[i]['ORDO_mappings']]\n",
    "        mapped_dict['SNOMED_ID'] += [SNOMED_code]\n",
    "        mapped_dict['SNOMED_term'] += [SNOMED_term]\n",
    "\n",
    "    else:\n",
    "        # Add term info to unmapped_dict\n",
    "        unmapped_dict['ORDO_ID'] += [UMLS_mapped.iloc[i]['ORDO_ID']]\n",
    "        unmapped_dict['ORDO_term'] += [UMLS_mapped.iloc[i]['ORDO_term']] \n",
    "        unmapped_dict['ORDO_syn'] += [UMLS_mapped.iloc[i]['ORDO_syn']]\n",
    "        unmapped_dict['ORDO_definition'] += [UMLS_mapped.iloc[i]['ORDO_definition']]\n",
    "        unmapped_dict['ORDO_mappings'] += [UMLS_mapped.iloc[i]['ORDO_mappings']]\n",
    "        unmapped_dict['SNOMED_ID'] += [\"\"]\n",
    "        unmapped_dict['SNOMED_term'] += [\"\"]\n",
    "                \n",
    "    counter += 1\n",
    "    if counter % 100 == 0:\n",
    "        runs_left = len(UMLS_mapped) - counter\n",
    "        print(\"{} runs left\".format(runs_left))\n",
    "\n",
    "\n",
    "mapped = pd.DataFrame.from_dict(mapped_dict)\n",
    "unmapped = pd.DataFrame.from_dict(unmapped_dict)\n",
    "unmapped = pd.concat([UMLS_unmapped, unmapped])\n",
    "\n",
    "print(\"Number of ORDO terms mapped to SNOMED through UMLS: \" + str(len(mapped)))\n",
    "print(\"Number of ORDO terms not mapped to UMLS or to SNOMED through UMLS: \" + str(len(unmapped)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "ordo_id = []\n",
    "concept = []\n",
    "score = []\n",
    "snomedid = []\n",
    "snomedterm = []\n",
    "for i in range(len(mapped.ORDO_ID.tolist())):\n",
    "    ordo = mapped.ORDO_ID.tolist()[i]\n",
    "    ordo = ordo.split('/')[-1].split('_')[1]\n",
    "    ordo_id.append(ordo)\n",
    "    score.append(1000)\n",
    "    concept.append(mapped.ORDO_term.tolist()[i])\n",
    "    snomedid.append(mapped.SNOMED_ID.tolist()[i])\n",
    "    snomedterm.append(mapped.SNOMED_term.tolist()[i])\n",
    "df_Mapped_ORDO_SNOMED_UMLS = pd.DataFrame({'ORDO-ID': ordo_id, \n",
    "                                      'Concept':concept,\n",
    "                                     'Mapped_Term': concept,\n",
    "                                     'SNOMED_ID': snomedid,\n",
    "                                     'SNOMED_Term': snomedterm,\n",
    "                                     'Score': score})\n",
    "df_Mapped_ORDO_SNOMED_UMLS.to_csv(\"Results/Mapped_Results/Mapped_ORDO_SNOMED_UMLS.txt\",sep=\"\\t\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "all_need_metamap = unmapped[['ORDO_ID','ORDO_term','ORDO_syn']]\n",
    "ordo_id = []\n",
    "for i in range(len(all_need_metamap.ORDO_ID.tolist())):\n",
    "    ordo = all_need_metamap.ORDO_ID.tolist()[i]\n",
    "    ordo = ordo.split('/')[-1].split('_')[1]\n",
    "    ordo_id.append(ordo)\n",
    "all_need_metamap = all_need_metamap.reset_index()\n",
    "del all_need_metamap['index']\n",
    "all_need_metamap['ORDO_ID'] = ordo_id\n",
    "all_need_metamap = all_need_metamap.fillna(0)\n",
    "all_need_metamap.to_csv(\"ORDO_info/all_need_metamap.txt\",sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A. Transform the 1424 Concepts into utf8 Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "!java -jar replace_utf8.jar ORDO_info/all_need_metamap.txt > ORDO_info/all_need_metamap.txt.utf8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B. Make 6 Dictionaries for Concepts, IDs and Synonyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_metamap = pd.read_csv('ORDO_info/all_need_metamap.txt.utf8', sep='\\t')\n",
    "\n",
    "#dict_id_concept is the dictionary with \n",
    "        #the key of id, and value of concept\n",
    "dict_id_concept = dict(zip(df_all_metamap.ORDO_ID,df_all_metamap.ORDO_term))\n",
    "#dict_concept_id is the dictionary with \n",
    "        #the key of concept, and value of id\n",
    "dict_concept_id = dict(zip(df_all_metamap.ORDO_term,df_all_metamap.ORDO_ID))\n",
    "#dict_concept_syn is the dictionary with\n",
    "        #the key of concept, and the value of syn list\n",
    "dict_concept_syn = {}\n",
    "#dict_id_syn is the dictionary with\n",
    "    #the key of id, and the value of syn list\n",
    "dict_id_syn = {}\n",
    "#dict_syn_concept is the dictionary with\n",
    "        # the key of each syn terms, and the value of corresponding concept\n",
    "dict_syn_concept = {}\n",
    "#dict_syn_id is the dictionary with\n",
    "        # the key of each syn terms, and the value of corresponding id\n",
    "dict_syn_id = {}\n",
    "#because some do not have syn, so only keep the one has syns in the dict\n",
    "for i in range(len(list(df_all_metamap.ORDO_ID))):\n",
    "    concept = list(df_all_metamap.ORDO_term)\n",
    "    synonym = list(df_all_metamap.ORDO_syn)\n",
    "    ordo_id = list(df_all_metamap.ORDO_ID)\n",
    "    cpt = concept[i]\n",
    "    syn = synonym[i].split('|')\n",
    "    o_id = ordo_id[i]\n",
    "    if cpt not in dict_concept_syn:\n",
    "        if '0' not in syn:\n",
    "            dict_concept_syn[cpt] = syn\n",
    "    if o_id not in dict_id_syn:\n",
    "        if '0' not in syn:\n",
    "            dict_id_syn[o_id] = syn\n",
    "    if '0' not in syn:\n",
    "        for syn_term in syn:\n",
    "            if syn_term not in dict_syn_concept:\n",
    "                dict_syn_concept[syn_term] = cpt\n",
    "            if syn_term not in dict_syn_id:\n",
    "                dict_syn_id[syn_term] = o_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is the number of concepts: 1424\n",
      "Here is the total number of the synonyms for these 1424 concepts: 2122\n"
     ]
    }
   ],
   "source": [
    "print(\"Here is the number of concepts: \"+ str(len(dict_concept_id)))\n",
    "print(\"Here is the total number of the synonyms for these 1424 concepts: \"+ str(len(dict_syn_concept)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C. Prepare the Relationship Between Snomed-terms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1) Get the snomed_term information form 'sct2_Relationship_Full' file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1011286, 10)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_relationship = pd.read_csv('Vocabularies/SnomedCT_USEditionRF2_PRODUCTION_20190901T120000Z/Full/Terminology/sct2_Relationship_Full_US1000124_20190901.txt',sep='\\t')\n",
    "df_relationship = df_relationship[df_relationship.active != 0]\n",
    "df_relationship_is = df_relationship.copy(deep = True)\n",
    "df_relationship_is = df_relationship_is[df_relationship_is['typeId'] == 116680003]\n",
    "df_relationship_is.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2) Filter Out the Ordo_terms that Contains ICD information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "!java -jar replace_utf8.jar ORDO_info/UMLS-unmapped.txt > ORDO_info/UMLS-unmapped.txt.utf8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_icd_terms(file):\n",
    "    everything = pd.read_csv(file, sep='\\t')\n",
    "    IDs = everything[\"ORDO_ID\"].str.extract('_([0-9]+)')\n",
    "    everything[\"ORDO_ID\"] = IDs.values.astype(int)\n",
    "    everything = everything[everything.ORDO_mappings.str.contains(\"ICD-10:\")]\n",
    "    ICD_list = []\n",
    "    for items in list(everything.ORDO_mappings):\n",
    "        items = items.split('|')\n",
    "        multi = []\n",
    "        for item in items:\n",
    "            if item.startswith(\"ICD\"):\n",
    "                multi.append(item.split(\":\")[1])\n",
    "        ICD_list.append(\";\".join(multi))\n",
    "    everything.ORDO_mappings = ICD_list\n",
    "    new_dataframe = everything[[\"ORDO_ID\",\"ORDO_term\", \"ORDO_syn\", \"ORDO_mappings\"]]\n",
    "    new_dataframe.columns = [\"OR-ID\", \"Concepts\", \"Synonyms\", \"ICD-ID\"]\n",
    "    new_dataframe.to_csv(\"ORDO_info/unmapped_ORDO_with_ICD.txt.utf8\", sep = '\\t')\n",
    "    return new_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"ORDO_info/UMLS-unmapped.txt.utf8\"\n",
    "df_Ordo_ICD = extract_icd_terms(file_path)\n",
    "concept_icd = dict(zip(df_Ordo_ICD.Concepts, df_Ordo_ICD['ICD-ID']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (3) Get the Relationship of Snomed Term's Concept, ID and Semantic Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "description_file = \"Vocabularies/SnomedCT_USEditionRF2_PRODUCTION_20190901T120000Z/Full/Terminology/sct2_Description_Full-en_US1000124_20190901.txt\"\n",
    "df_description = pd.read_csv(description_file,sep='\\t')\n",
    "df_description = df_description[df_description.active != 0]\n",
    "df_description = df_description[['conceptId','typeId','term']]\n",
    "df_description = df_description[df_description.typeId == 900000000000003001]\n",
    "df_description[\"Pharse\"] = df_description.term.str.extract(\"(\\([^\\)]*\\)$)\")\n",
    "df_description = df_description[df_description.Pharse.notnull()]\n",
    "snomedid_pharse = dict(zip(df_description.conceptId,df_description.Pharse))\n",
    "snomedid_pharse[10743271000119104]='(disorder)'\n",
    "snomedid_term = dict(zip(df_description.conceptId,df_description.term))\n",
    "snomedid_term[10743271000119104]='Immunoglobulin G4 related disease (disorder)'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "snomedid_pharse[723363009] = '(disorder)'\n",
    "snomedid_term[723363009] = 'Hypotrichosis, lymphedema, telangiectasia, renal defect syndrome (disorder)'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# II. MetaMap for String Match"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A. Metamap for the 1424 Concept, Only with the Parameter (WSD + SNOMED )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### (1) Output the file that contains 1424 concepts for metamapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"Results/concepts_wsd.txt.utf8\",'w') as output_file:\n",
    "    for key in list(df_all_metamap.ORDO_term):\n",
    "        output_file.write(key+'\\n')\n",
    "output_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### (2) Run the local MetaMap \n",
    "install information: https://metamap.nlm.nih.gov/Installation.shtml\n",
    "###### ! ./Vocabularies/public_mm/bin/skrmedpostctl start\n",
    "###### ! ./Vocabularies/public_mm/bin/wsdserverctl start\n",
    "###### ! python2 Vocabularies/public_mm/call_metamap.py Results/concepts_wsd.txt.utf8 Results/concepts_wsd_out.txt.utf8 concept_metamap_result/json  Vocabularies/public_mm/bin/metamap18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting skrmedpostctl: \r\n",
      "started.\r\n"
     ]
    }
   ],
   "source": [
    "! ./Vocabularies/public_mm/bin/skrmedpostctl start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting wsdserverctl: \r\n",
      "started.\r\n"
     ]
    }
   ],
   "source": [
    "! ./Vocabularies/public_mm/bin/wsdserverctl start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! python2 Vocabularies/public_mm/call_metamap.py Results/concepts_wsd.txt.utf8 Results/concepts_wsd_out.txt.utf8 Results/concepts_metamap_result/json Vocabularies/public_mm/bin/metamap18 '-AIyf -R SNOMEDCT_US -J bpoc,clas,clna,cgab,dsyn,fndg,ftcn,genf,mobd,neop,ortf,phsf,qlco,qnco,spco,tmco --JSONf 2 -V USAbase'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### (3) Read the Metamap Result in json Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "## check if there is mapping in the metamap output\n",
    "def mapping_exist(metamap):\n",
    "    phrases = metamap[\"AllDocuments\"][0][\"Document\"][\"Utterances\"][0][\"Phrases\"]\n",
    "    has_mapping = False\n",
    "    for phrase in phrases:\n",
    "        if len(phrase[\"Mappings\"]) != 0:\n",
    "            has_mapping = True\n",
    "    return has_mapping\n",
    "\n",
    "## check if there is mapping for every phrase in the metamap output\n",
    "def mapping_exist_for_all_phrases(metamap):\n",
    "    phrases = metamap[\"AllDocuments\"][0][\"Document\"][\"Utterances\"][0][\"Phrases\"]\n",
    "    has_mapping = True\n",
    "    for phrase in phrases:\n",
    "        if len(phrase[\"Mappings\"]) == 0:\n",
    "            has_mapping = False\n",
    "    return has_mapping\n",
    "\n",
    "## check if a mapping has multiple candidates\n",
    "def multiple_candidates(mapping):\n",
    "    for candidate in mapping[\"MappingCandidates\"]:\n",
    "        if mapping[\"MappingScore\"] != candidate[\"CandidateScore\"]:\n",
    "            #print(\"major problem!!!\")\n",
    "            #print(mapping)\n",
    "            pass\n",
    "    return (len(mapping[\"MappingCandidates\"]) > 1)\n",
    "## count match terms\n",
    "def count_match(phrase):\n",
    "    \n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    result = tokenizer.tokenize(phrase[\"PhraseText\"])\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    new_phrase =[]\n",
    "    \n",
    "    for w in result:\n",
    "        if w not in stop_words: new_phrase.append(w)\n",
    "\n",
    "    num_of_words = len(new_phrase)\n",
    "    \n",
    "    num_of_matches = 0\n",
    "    for mapping in phrase[\"Mappings\"]:\n",
    "        for candidate in mapping[\"MappingCandidates\"]:\n",
    "            num_of_matches += len(candidate[\"MatchedWords\"])\n",
    "    \n",
    "    return (num_of_matches, num_of_words)\n",
    "\n",
    "def read_metamap_result_json(file,num):\n",
    "    all_concept_list = []\n",
    "    all_CUI_list = []\n",
    "    all_score_list = []\n",
    "    all_term_list = []\n",
    "    all_exist = 0\n",
    "    exist = 0\n",
    "    for i in range(0,num):\n",
    "        file_to_read = file +'json%d.txt'%(i+1)\n",
    "        with open(file_to_read) as file_handler:\n",
    "            CUI_list = []\n",
    "            score_list = []\n",
    "            term_list = []\n",
    "            file_handler.readline()\n",
    "            metamap = json.load(file_handler)\n",
    "            all_concept_list.append(metamap[\"AllDocuments\"][0][\"Document\"][\"Utterances\"][0][\"UttText\"])\n",
    "    \n",
    "            if mapping_exist(metamap):\n",
    "                exist += 1\n",
    "            \n",
    "            if mapping_exist_for_all_phrases(metamap):\n",
    "                all_exist += 1\n",
    "                phrases = metamap[\"AllDocuments\"][0][\"Document\"][\"Utterances\"][0][\"Phrases\"]\n",
    "                matches, words = 0, 0\n",
    "                for phrase in phrases:\n",
    "                    phrase_matches, phrase_words = count_match(phrase)\n",
    "                    matches, words = matches + phrase_matches, words + phrase_words\n",
    "                    \n",
    "                    phrase_CUI_list = []\n",
    "                    phrase_score_list = []\n",
    "                    phrase_term_list = []\n",
    "                    mappings = phrase[\"Mappings\"]\n",
    "                    fraxe = 0\n",
    "                    for mapping in mappings:\n",
    "                        candidates = mapping[\"MappingCandidates\"]\n",
    "                        for candidate in candidates:\n",
    "                            if candidate[\"CandidateMatched\"] == \"FRAXE\":\n",
    "                                fraxe = 1\n",
    "                            phrase_CUI_list.append(candidate[\"CandidateCUI\"])\n",
    "                            phrase_score_list.append(candidate[\"CandidateScore\"])\n",
    "                            phrase_term_list.append(candidate[\"CandidateMatched\"])\n",
    "                                            \n",
    "                    CUI_list.append(phrase_CUI_list)\n",
    "                    score_list.append(phrase_score_list)\n",
    "                    term_list.append(phrase_term_list)\n",
    "                    if fraxe == 1:\n",
    "                        pass\n",
    "                        #print(phrase_CUI_list)\n",
    "                if matches/words < 0.5:\n",
    "                    CUI_list = []\n",
    "                    score_list = []\n",
    "                    term_list = []\n",
    "                \n",
    "        all_CUI_list.append(CUI_list)\n",
    "        all_score_list.append(score_list)\n",
    "        all_term_list.append(term_list)\n",
    "    return all_concept_list,all_CUI_list,all_score_list,all_term_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is the number of concept that has 1000 score in Metamap: 262\n"
     ]
    }
   ],
   "source": [
    "file_path = 'Results/concepts_metamap_result/'\n",
    "num_of_terms = len(os.listdir(\"Results/concepts_metamap_result/\"))\n",
    "all_concept_list,all_CUI_list,all_score_list,all_term_list = read_metamap_result_json(file_path,num_of_terms)\n",
    "df_concept_metamap = pd.DataFrame({\"Concepts\": all_concept_list, \"Matched_CUIs\": all_CUI_list, \"Matched_Scores\": all_score_list, \"Matched_Terms\": all_term_list})\n",
    "df_concept_metamap_1000 = df_concept_metamap[df_concept_metamap.Matched_Scores.map(lambda d: d == [['-1000']])]\n",
    "print(\"Here is the number of concept that has 1000 score in Metamap: \"+str(len(df_concept_metamap_1000.Concepts.tolist())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### (4) Make a dictionary of mapped concepts with corresponding cui;  Save the cuis to these concept to a file \"cui_concept_list.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapped_cui = []\n",
    "mapped_term = []\n",
    "for cuis in df_concept_metamap_1000.Matched_CUIs.tolist():\n",
    "    mapped_cui.append(cuis[0][0])\n",
    "for terms in df_concept_metamap_1000.Matched_Terms.tolist():\n",
    "    mapped_term.append(terms[0][0])\n",
    "mapped_concept_cui = dict(zip(df_concept_metamap_1000.Concepts, mapped_cui))\n",
    "mapped_concept_term = dict(zip(df_concept_metamap_1000.Concepts, mapped_term))\n",
    "with open(\"Results/cui_concept_list.txt\",'w') as output_file:\n",
    "    for key in mapped_concept_cui:\n",
    "        cui = mapped_concept_cui[key]\n",
    "        output_file.write(key + '\\t' + cui+'\\n')\n",
    "output_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B. Metamap for the Synonyms of the rest 1156 Concept, Only with the Parameter (WSD + SNOMED )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### (1) Output the file that only contains the synonyms of the rest 1156 concept  for metamapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_remain_concept(df):\n",
    "    remain_concept_synonyms = []\n",
    "    for concept in df.ORDO_term.tolist():\n",
    "        remain_concept_synonyms.append(concept)\n",
    "        if concept in dict_concept_syn:\n",
    "            synonyms = dict_concept_syn[concept]\n",
    "            for synonym in synonyms:\n",
    "                remain_concept_synonyms.append(synonym)\n",
    "    return remain_concept_synonyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_remain_synonyms(df):\n",
    "    remain_concept_synonyms = []\n",
    "    for concept in df.ORDO_term.tolist():\n",
    "        if concept in dict_concept_syn:\n",
    "            synonyms = dict_concept_syn[concept]\n",
    "            for synonym in synonyms:\n",
    "                remain_concept_synonyms.append(synonym)\n",
    "    return remain_concept_synonyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is the total number of synonyms for metmap: 1662\n"
     ]
    }
   ],
   "source": [
    "df_concept_metamap_fail = df_all_metamap[~df_all_metamap.ORDO_term.map(lambda d: d in df_concept_metamap_1000.Concepts.tolist() )]\n",
    "synonyms_wsd = get_remain_synonyms(df_concept_metamap_fail)\n",
    "print(\"Here is the total number of synonyms for metmap: \"+ str(len(synonyms_wsd)))\n",
    "\n",
    "with open(\"Results/synonyms_wsd.txt.utf8\",'w') as output_file:\n",
    "    for key in synonyms_wsd:\n",
    "        output_file.write(key + '\\n')\n",
    "output_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### (2) Run the local MetaMap \n",
    "\n",
    "###### ! python2 Vocabularies/public_mm/call_metamap.py Results/synonyms_wsd_out.txt.utf8 Results/synonyms_wsd.txt.utf8 Results/synonyms_metamap_result/json Vocabularies/public_mm/bin/metamap18 '-AIyf -R SNOMEDCT_US -J bpoc,clas,clna,cgab,dsyn,fndg,ftcn,genf,mobd,neop,ortf,phsf,qlco,qnco,spco,tmco --JSONf 2 -V USAbase'\n",
    "\n",
    "###### argv[1] the file call metamap\n",
    "###### argv[2] the file to run metamap\n",
    "###### argv[3] the output overall file\n",
    "###### argv[4] the where the outfiles of each term in the input file(argv[2]) stored, and named as json#\n",
    "###### argv[5] the parameter for metamap\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! python2 Vocabularies/public_mm/call_metamap.py Results/synonyms_wsd.txt.utf8 Results/synonyms_wsd_out.txt.utf8 Results/synonyms_metamap_result/json Vocabularies/public_mm/bin/metamap18 '-AIyf -R SNOMEDCT_US -J bpoc,clas,clna,cgab,dsyn,fndg,ftcn,genf,mobd,neop,ortf,phsf,qlco,qnco,spco,tmco --JSONf 2 -V USAbase'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### (3) Read the Metamap Result in json Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is the number of synonyms that has 1000 score in Metamap: 202\n"
     ]
    }
   ],
   "source": [
    "file_path = 'Results/synonyms_metamap_result/'\n",
    "num_of_terms = len(os.listdir('Results/synonyms_metamap_result'))\n",
    "all_concept_list,all_CUI_list,all_score_list,all_term_list = read_metamap_result_json(file_path,num_of_terms)\n",
    "df_synonyms_metamap = pd.DataFrame({\"Concepts\": all_concept_list, \"Matched_CUIs\": all_CUI_list, \"Matched_Scores\": all_score_list, \"Matched_Terms\": all_term_list})\n",
    "df_synonyms_metamap_1000 = df_synonyms_metamap[df_synonyms_metamap.Matched_Scores.map(lambda d: d == [['-1000']])]\n",
    "print(\"Here is the number of synonyms that has 1000 score in Metamap: \"+str(len(df_synonyms_metamap_1000.Concepts.tolist())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is the number of concepts that has 1000 score in Metamap with its synonyms: 138\n"
     ]
    }
   ],
   "source": [
    "mapped_concept_by_syn = []\n",
    "for syn in df_synonyms_metamap_1000.Concepts.tolist():\n",
    "    if syn in dict_syn_concept:\n",
    "        cpt = dict_syn_concept[syn]\n",
    "        mapped_concept_by_syn.append(cpt)\n",
    "    else:\n",
    "        print(\"Error\")\n",
    "        print(syn)\n",
    "mapped_concept_by_syn = list(set(mapped_concept_by_syn))\n",
    "print(\"Here is the number of concepts that has 1000 score in Metamap with its synonyms: \"+str(len(mapped_concept_by_syn)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### (4) Make a dictionary of mapped concepts through synonyms with corresponding cui; Save the cuis to these concept to a file \"cui_synonyms_list.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapped_cui = []\n",
    "mapped_term = []\n",
    "for cuis in df_synonyms_metamap_1000.Matched_CUIs.tolist():\n",
    "    mapped_cui.append(cuis[0][0])\n",
    "for terms in df_synonyms_metamap_1000.Matched_Terms.tolist():\n",
    "    mapped_term.append(terms[0][0])\n",
    "mapped_synonym_cui = dict(zip(df_synonyms_metamap_1000.Concepts, mapped_cui))\n",
    "mapped_synonym_term = dict(zip(df_synonyms_metamap_1000.Concepts, mapped_term))\n",
    "with open(\"Results/cui_synonym_list.txt\",'w') as output_file:\n",
    "    for key in mapped_synonym_cui:\n",
    "        cui = mapped_synonym_cui[key]\n",
    "        output_file.write(key + '\\t' + cui+'\\n')\n",
    "output_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C. Filter Out the MetMap Mapping without SNOMED_IDs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### (1) Concat the Two cui_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "! cat Results/cui_concept_list.txt Results/cui_synonym_list.txt > Results/all_cui_metamap_list.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### (2) Get the SNOMED_ID for each CUI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def UMLS_to_SNOMED(file):\n",
    "    df = pd.read_csv(file, names=[\"Concept\",\"UMLS_CUI\"], sep='\\t')\n",
    "    # UMLS API requires getting TGT every 8 hours: https://documentation.uts.nlm.nih.gov/rest/authentication.html\n",
    "    # Or just get every run (below)\n",
    "    headers = {\"Content-Type\": \"application/x-www-form-urlencoded\"}\n",
    "\n",
    "    user = input(\"Please enter your username: \")\n",
    "    pw = getpass.getpass(\"Please enter your password: \")\n",
    "    params = {\"username\" : user,\n",
    "             \"password\" : pw}\n",
    "\n",
    "    TGT_URL = \"https://utslogin.nlm.nih.gov/cas/v1/tickets\"\n",
    "\n",
    "    response = (requests.post(TGT_URL, headers = headers, params = params)).text\n",
    "    ticketgetter = BeautifulSoup(response, features=\"lxml\")\n",
    "    TGT = ticketgetter.form['action']\n",
    "\n",
    "    # For service ticket request\n",
    "    headers_ST = {\"Content-Type\": \"application/x-www-form-urlencoded\"}\n",
    "    params = {\"service\": \"http://umlsks.nlm.nih.gov\"}\n",
    "\n",
    "    # Check CUIs for each mapping in UMLS_mapped\n",
    "    counter = 0\n",
    "    Slist = []\n",
    "    for i, CUI in enumerate(df['UMLS_CUI']):\n",
    "\n",
    "        # Request UMLS concept info from API\n",
    "        # Every request requires a service ticket (use TGT to get service ticket)\n",
    "        ST = requests.post(TGT, headers = headers_ST, params = params)\n",
    "        URL = \"https://uts-ws.nlm.nih.gov/rest/content/current/CUI/{}/atoms?sabs=SNOMEDCT_US&ticket={}\".format(CUI, ST.text)\n",
    "        response = requests.get(URL)\n",
    "\n",
    "        # Check if SNOMED-CT is mapped; get SNOMED ID\n",
    "        if response.text[0] == \"{\":     # \"{\" indicates start of response with AUIs; \"<\" indicates page not found (no AUIs)\n",
    "            atom_dict = json.loads(response.text)\n",
    "            atom_list = atom_dict['result']\n",
    "            # Atom list contains 2 SNOMED atoms, but both are same concept/code. Can use second one which is Fully-specified name\n",
    "            atom = atom_list[0]\n",
    "            SNOMED_term = atom['name']\n",
    "            SNOMED_code = atom['code'].rsplit('/', 1)[1]\n",
    "            # Add term info to mapped_dict\n",
    "            Slist.append(SNOMED_code)\n",
    "\n",
    "        else:\n",
    "            # Add term info to unmapped_dict\n",
    "            Slist.append(\"\")\n",
    "\n",
    "        counter += 1\n",
    "        if counter % 100 == 0:\n",
    "            runs_left = len(df[\"UMLS_CUI\"]) - counter\n",
    "            print(\"{} runs left\".format(runs_left))\n",
    "    df[\"SNOMED\"] = Slist\n",
    "    return df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please enter your username: ilyia1997\n",
      "Please enter your password: ········\n",
      "364 runs left\n",
      "264 runs left\n",
      "164 runs left\n",
      "64 runs left\n"
     ]
    }
   ],
   "source": [
    "file_path = \"Results/all_cui_metamap_list.txt\"\n",
    "df_cui_snomed_metamap =  UMLS_to_SNOMED(file_path)\n",
    "df_cui_snomed_metamap.to_csv('Results/cui_snomed_metamap.txt', sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### (3) Read the File that has the SNOMED_ID Mapped to the CUIs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Concept</th>\n",
       "      <th>UMLS_CUI</th>\n",
       "      <th>SNOMED</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>173</th>\n",
       "      <td>Ameloblastic carcinoma</td>\n",
       "      <td>C1314678</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>Loeffler endocarditis</td>\n",
       "      <td>C0206143</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>259</th>\n",
       "      <td>Hereditary diffuse gastric cancer</td>\n",
       "      <td>C1708349</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>320</th>\n",
       "      <td>Familial cerebral amyloid angiopathy</td>\n",
       "      <td>C0268393</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>442</th>\n",
       "      <td>Familial Alzheimer disease</td>\n",
       "      <td>C0276496</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>454</th>\n",
       "      <td>WDM</td>\n",
       "      <td>C0221054</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  Concept  UMLS_CUI  SNOMED\n",
       "173                Ameloblastic carcinoma  C1314678     NaN\n",
       "196                 Loeffler endocarditis  C0206143     NaN\n",
       "259     Hereditary diffuse gastric cancer  C1708349     NaN\n",
       "320  Familial cerebral amyloid angiopathy  C0268393     NaN\n",
       "442            Familial Alzheimer disease  C0276496     NaN\n",
       "454                                   WDM  C0221054     NaN"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_cui_snomed_metamap = pd.read_csv('Results/cui_snomed_metamap.txt',sep='\\t')\n",
    "del df_cui_snomed_metamap['Unnamed: 0']\n",
    "df_cui_snomed_metamap[df_cui_snomed_metamap.SNOMED.isnull()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### (4) Save the Mapped Concept to Mapped_Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is the total number of concepts that mapped by MetaMap Score of 1000: 394\n"
     ]
    }
   ],
   "source": [
    "snomed_inactive = [801000000000,107691000000000,26261000000000,91521000000000]\n",
    "\n",
    "df_cui_snomed_metamap = df_cui_snomed_metamap[df_cui_snomed_metamap.SNOMED.notnull()]\n",
    "df_cui_snomed_metamap.SNOMED = df_cui_snomed_metamap.SNOMED.astype(int)\n",
    "\n",
    "df_cui_snomed_metamap = df_cui_snomed_metamap[~df_cui_snomed_metamap.SNOMED.map(lambda d: d in snomed_inactive)]\n",
    "\n",
    "cptsyn_with_snomed = dict(zip(df_cui_snomed_metamap.Concept, df_cui_snomed_metamap.SNOMED))\n",
    "mapped_term = []\n",
    "mapped_concept = []\n",
    "mapped_cui = []\n",
    "mapped_snomedid = []\n",
    "mapped_snomedterm = []\n",
    "mapped_score = []\n",
    "mapped_ordoid = []\n",
    "for i in range(len(df_cui_snomed_metamap.Concept.tolist())):\n",
    "    mappedterm = df_cui_snomed_metamap.Concept.tolist()[i]\n",
    "    if mappedterm in list(df_all_metamap.ORDO_term):\n",
    "        cpt = mappedterm\n",
    "    elif mappedterm in dict_syn_concept:\n",
    "        cpt = dict_syn_concept[mappedterm]\n",
    "    else:\n",
    "        print('Error')\n",
    "    ordoid = dict_concept_id[cpt]\n",
    "    cui = df_cui_snomed_metamap.UMLS_CUI.tolist()[i]\n",
    "    snomedid = df_cui_snomed_metamap.SNOMED.tolist()[i]\n",
    "    snomedterm = snomedid_term[int(snomedid)]\n",
    "    score = 1000\n",
    "    mapped_term.append(mappedterm)\n",
    "    mapped_concept.append(cpt)\n",
    "    mapped_cui.append(cui)\n",
    "    mapped_snomedid.append(snomedid)\n",
    "    mapped_snomedterm.append(snomedterm)\n",
    "    mapped_score.append(score)\n",
    "    mapped_ordoid.append(ordoid)\n",
    "df_metamap_1000_mapped = pd.DataFrame({'ORDO_ID': mapped_ordoid,\n",
    "                                      'Concept': mapped_concept,\n",
    "                                      'Mapped_Term': mapped_term,\n",
    "                                      'SNOMED_ID': mapped_snomedid,\n",
    "                                      'SNOMED_Term': mapped_snomedterm,\n",
    "                                      'Score': mapped_score})\n",
    "df_metamap_1000_mapped = df_metamap_1000_mapped.sort_values('ORDO_ID', ascending = False)\n",
    "df_metamap_1000_mapped = df_metamap_1000_mapped.drop_duplicates(subset = 'Concept', keep = 'first')\n",
    "df_metamap_1000_mapped.to_csv(\"Results/Mapped_Results//Mapped_Results_Metamap1000.txt\",sep=\"\\t\")\n",
    "print(\"Here is the total number of concepts that mapped by MetaMap Score of 1000: \"+ str(len(df_metamap_1000_mapped.Concept.tolist())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## D. For the Remaining 1029 Concepts, Fuzzy Mapping them and their Synonyms by ICD-SNOMED mapping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### (1) Get the Concepts have ICD Mapping from the Remaining 1029 Concepts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is the number of concepts from the remaining 1023 that has icd mapping: 742\n"
     ]
    }
   ],
   "source": [
    "concept_with_icd = df_Ordo_ICD.Concepts.tolist()\n",
    "df_concept_metamap_fail = df_all_metamap[~df_all_metamap.ORDO_term.map(lambda d: d in df_metamap_1000_mapped.Concept.tolist() )]\n",
    "df_fuzzymap = df_concept_metamap_fail[df_concept_metamap_fail.ORDO_term.map(lambda d: d in concept_with_icd)]\n",
    "del df_fuzzymap['Unnamed: 0']\n",
    "df_fuzzymap.head()\n",
    "df_fuzzymap.columns = ['OR-ID','Concepts','Synonyms']\n",
    "print(\"Here is the number of concepts from the remaining 1023 that has icd mapping: \"+str(len(df_fuzzymap.Concepts.tolist())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### (2) Keep the Concept with ICD-SNOMED Mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('ORDO_info/ICD_SNOMED_mappings.json','r') as f:\n",
    "    dict_icd_snomed = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### (3) Get the Fuzzy Mapping Terms "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "292\n"
     ]
    }
   ],
   "source": [
    "ICD_list = []\n",
    "for concept in df_fuzzymap['Concepts'].tolist():\n",
    "    icd = concept_icd[concept]\n",
    "    ICD_list.append(icd)\n",
    "df_fuzzymap = df_fuzzymap.reset_index()\n",
    "del df_fuzzymap['index']\n",
    "df_fuzzymap['ICD-ID'] = ICD_list\n",
    "\n",
    "df_ordo_snomed = df_fuzzymap.copy(deep=True)\n",
    "\n",
    "ICD_SNOME_NOMAP = []\n",
    "SNOMED_list=[]\n",
    "count=0\n",
    "ICD_list = df_ordo_snomed['ICD-ID'].tolist()\n",
    "for icd_id in ICD_list:\n",
    "    icd_ids = icd_id.split(';')\n",
    "    snomeds=''\n",
    "    for icd in icd_ids:\n",
    "        if icd in dict_icd_snomed:\n",
    "            snomed = dict_icd_snomed[icd]\n",
    "            if snomeds != '':\n",
    "                snomeds += ';'\n",
    "            snomeds += ';'.join(snomed)\n",
    "    \n",
    "    SNOMED_list.append(snomeds)\n",
    "df_ordo_snomed['SNOMED-ID'] = SNOMED_list\n",
    "print(sum(df_ordo_snomed['SNOMED-ID']==''))\n",
    "df_ordo_snomed_exist = df_ordo_snomed.copy(deep=True)\n",
    "df_ordo_snomed_exist = df_ordo_snomed_exist[df_ordo_snomed_exist['SNOMED-ID'] != '']\n",
    "df_ordo_snomed_notmap = df_ordo_snomed.copy(deep=True)\n",
    "df_ordo_snomed_notmap = df_ordo_snomed_notmap[df_ordo_snomed_notmap['SNOMED-ID'] == '']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(450, 5)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ordo_snomed_exist.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### (4) Fuzzy Mapping the 1705 terms (Concept + Synonyms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "description_file = \"Vocabularies/SnomedCT_USEditionRF2_PRODUCTION_20190901T120000Z/Full/Terminology/sct2_Description_Full-en_US1000124_20190901.txt\"\n",
    "df_description_1 = pd.read_csv(description_file,sep='\\t')\n",
    "df_description_1 = df_description_1[df_description_1.active != 0]\n",
    "df_description_1 = df_description_1[['conceptId','term']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def catch_children(sid):\n",
    "    sid_int = int(sid)\n",
    "    source = df_relationship_is.loc[df_relationship_is['destinationId']==sid_int]['sourceId']\n",
    "    return source.tolist()\n",
    "\n",
    "def find_all_chil(snomed_ids):\n",
    "    stack = LifoQueue(maxsize=9999)\n",
    "    snomed_id_list = snomed_ids.split(\";\")\n",
    "    for snomed_id in snomed_id_list:\n",
    "        stack.put(snomed_id)\n",
    "    everything = set()\n",
    "    while not stack.empty():\n",
    "        sid = stack.get()\n",
    "        if sid not in everything:\n",
    "            everything.add(sid)\n",
    "        else:\n",
    "            continue\n",
    "        source = catch_children(sid)\n",
    "        for item in source:\n",
    "            stack.put(str(item))\n",
    "    return ';'.join(everything)\n",
    "def find_snomed_id(term):\n",
    "    id_list = df_description_1.loc[df_description_1['term']==term,'conceptId'].tolist()\n",
    "    if len(id_list) == 0:\n",
    "        id_list.append(np.nan)\n",
    "    return  list(set(id_list))\n",
    "snomed_term_id_dic={}\n",
    "def SnomedID_to_Terms(SnomedID):\n",
    "    snomedIDs = SnomedID.split(\";\")\n",
    "    terms=[]\n",
    "    for snomedid in snomedIDs:\n",
    "        snomedid_int = int(snomedid)\n",
    "        term = df_description_1.loc[df_description_1['conceptId']==snomedid_int,'term'].tolist()\n",
    "        for t in term:\n",
    "            if t not in snomed_term_id_dic:\n",
    "                key = []\n",
    "                key.append(snomedid_int)\n",
    "                snomed_term_id_dic[t] = key\n",
    "                #print(t)\n",
    "            else:\n",
    "                #print(\"t in dic already\")\n",
    "                key = snomed_term_id_dic[t]\n",
    "                #print(key)\n",
    "                key.append(snomedid_int)\n",
    "                key = list(set(key))\n",
    "                snomed_term_id_dic[t]=key\n",
    "            terms.append(t)\n",
    "    return terms  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>OR-ID</th>\n",
       "      <th>Concepts</th>\n",
       "      <th>Synonyms</th>\n",
       "      <th>ICD-ID</th>\n",
       "      <th>SNOMED-ID</th>\n",
       "      <th>SNOMED_ALL</th>\n",
       "      <th>SNOMED_ALL_TERM</th>\n",
       "      <th>Best_match</th>\n",
       "      <th>Best_score</th>\n",
       "      <th>Best_match_of_choice_in_synorcon</th>\n",
       "      <th>Best_matched_snomed_id</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Concepts_or_Syn</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Synonyms</th>\n",
       "      <td>146</td>\n",
       "      <td>146</td>\n",
       "      <td>146</td>\n",
       "      <td>146</td>\n",
       "      <td>146</td>\n",
       "      <td>146</td>\n",
       "      <td>146</td>\n",
       "      <td>146</td>\n",
       "      <td>146</td>\n",
       "      <td>146</td>\n",
       "      <td>146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>concepts</th>\n",
       "      <td>304</td>\n",
       "      <td>304</td>\n",
       "      <td>304</td>\n",
       "      <td>304</td>\n",
       "      <td>304</td>\n",
       "      <td>304</td>\n",
       "      <td>304</td>\n",
       "      <td>304</td>\n",
       "      <td>304</td>\n",
       "      <td>304</td>\n",
       "      <td>304</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 OR-ID  Concepts  Synonyms  ICD-ID  SNOMED-ID  SNOMED_ALL  \\\n",
       "Concepts_or_Syn                                                             \n",
       "Synonyms           146       146       146     146        146         146   \n",
       "concepts           304       304       304     304        304         304   \n",
       "\n",
       "                 SNOMED_ALL_TERM  Best_match  Best_score  \\\n",
       "Concepts_or_Syn                                            \n",
       "Synonyms                     146         146         146   \n",
       "concepts                     304         304         304   \n",
       "\n",
       "                 Best_match_of_choice_in_synorcon  Best_matched_snomed_id  \n",
       "Concepts_or_Syn                                                            \n",
       "Synonyms                                      146                     146  \n",
       "concepts                                      304                     304  "
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SNOMED_ALL=[]\n",
    "for item in df_ordo_snomed_exist['SNOMED-ID'].tolist():\n",
    "    SNOMED_ALL.append(find_all_chil(item))\n",
    "df_ordo_snomed_exist['SNOMED_ALL'] = SNOMED_ALL\n",
    "\n",
    "\n",
    "\n",
    "SNOMED_ALL_TERM=[]\n",
    "for item in df_ordo_snomed_exist['SNOMED_ALL'].tolist():\n",
    "    SNOMED_ALL_TERM.append(SnomedID_to_Terms(item))\n",
    "df_ordo_snomed_exist['SNOMED_ALL_TERM'] = SNOMED_ALL_TERM\n",
    "\n",
    "concepts_list = df_ordo_snomed_exist['Concepts'].tolist()\n",
    "synonyms_list = df_ordo_snomed_exist['Synonyms'].tolist()\n",
    "con_syn_list = []\n",
    "for i in range(len(concepts_list)):\n",
    "    concept = concepts_list[i]\n",
    "    if synonyms_list[i] == '0':\n",
    "        synonyms = ['']\n",
    "    else:\n",
    "        synonyms = synonyms_list[i].split('|')\n",
    "    con_and_syn = []\n",
    "    con_and_syn.append(concept)\n",
    "    for j in range(len(synonyms)):\n",
    "        con_and_syn.append(synonyms[j])\n",
    "    con_syn_list.append(con_and_syn)\n",
    "snomed_term_list= df_ordo_snomed_exist['SNOMED_ALL_TERM'].tolist()\n",
    "Best_score=[]\n",
    "Best_match=[]\n",
    "Best_match_synorcon=[]\n",
    "Best_matched_snomed_id=[]\n",
    "for i in range(len(con_syn_list)):\n",
    "    con_and_syn = con_syn_list[i]\n",
    "    max_score = 0\n",
    "    max_term = None\n",
    "    con_syn_choice = None\n",
    "    best_snm_id = None\n",
    "    for concept in con_and_syn:\n",
    "        options = snomed_term_list[i]\n",
    "        for term in options:\n",
    "            score = fuzz.token_sort_ratio(concept,term)\n",
    "            if score >= max_score:\n",
    "                max_score = score\n",
    "                max_term = term\n",
    "                con_syn_choice = concept\n",
    "                        \n",
    "    Best_score.append(max_score)\n",
    "    Best_match.append(max_term)\n",
    "    Best_match_synorcon.append(con_syn_choice)\n",
    "\n",
    "df_ordo_snomed_exist['Best_match']=Best_match\n",
    "df_ordo_snomed_exist['Best_score']=Best_score\n",
    "df_ordo_snomed_exist['Best_match_of_choice_in_synorcon']=Best_match_synorcon\n",
    "for matches in Best_match:\n",
    "    if matches in snomed_term_id_dic:\n",
    "        Best_matched_snomed_id.append(snomed_term_id_dic[matches])\n",
    "    else:\n",
    "        Best_matched_snomed_id.append(np.nan)\n",
    "df_ordo_snomed_exist['Best_matched_snomed_id']=Best_matched_snomed_id\n",
    "df_ordo_snomed_exist['Concepts_or_Syn'] = np.where(df_ordo_snomed_exist['Best_match_of_choice_in_synorcon']==df_ordo_snomed_exist['Concepts'], 'concepts', 'Synonyms')\n",
    "df_ordo_snomed_exist.groupby('Concepts_or_Syn').count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### (5) Output the Concept that has Fuzzy score > 80"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is the number of concept has fuzzy score over 80: 277\n"
     ]
    }
   ],
   "source": [
    "df_stringmatch_80 = df_ordo_snomed_exist[['OR-ID','Concepts','Best_match_of_choice_in_synorcon','Best_matched_snomed_id','Best_match','Best_score']]\n",
    "df_stringmatch_80 = df_stringmatch_80[df_ordo_snomed_exist.Best_score.map(lambda d: d >=80)]\n",
    "df_stringmatch_80.columns = ['ORDO-ID','Concept','Mapped_Term','SNOMED_ID','SNOMED_Term','Score']\n",
    "snomed_id_list = []\n",
    "for snomed in df_stringmatch_80.SNOMED_ID:\n",
    "    snomed_id_list.append(snomed[0])\n",
    "df_stringmatch_80['SNOMED_ID'] = snomed_id_list\n",
    "df_stringmatch_80.to_csv(\"Results/Mapped_Results//Mapped_Results_FuzzyMap80.txt\",sep=\"\\t\")\n",
    "print(\"Here is the number of concept has fuzzy score over 80: \" + str(len(snomed_id_list)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# III MetaMap for Post-Coordinations "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A. Get the remaining 750 Concepts and their Synonyms for MetaMap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is the total number of concepts and synonyms run for metamap: 1590\n"
     ]
    }
   ],
   "source": [
    "df_concept_metamap_fail = df_all_metamap[~df_all_metamap.ORDO_term.map(lambda d: d in df_metamap_1000_mapped.Concept.tolist() )]\n",
    "df_concept_stringmatch_fail = df_concept_metamap_fail[~df_concept_metamap_fail.ORDO_term.map(lambda d: d in df_stringmatch_80.Concept.tolist())]\n",
    "remain_concept_synonyms = get_remain_concept(df_concept_stringmatch_fail)\n",
    "print(\"Here is the total number of concepts and synonyms run for metamap: \"+ str(len(remain_concept_synonyms)))\n",
    "with open(\"Results/concept_n_synonyms_for_post.txt.utf8\",'w') as output_file:\n",
    "    for key in remain_concept_synonyms:\n",
    "        output_file.write(key + '\\n')\n",
    "output_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B. Run the MetaMap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#! python2 Vocabularies/public_mm/call_metamap.py Results/concept_n_synonyms_for_post.txt.utf8 Results/concept_n_synonyms_for_post_out.txt.utf8 Results/concepts_and_synonyms_metamap_for_postCoordination_result/json Vocabularies/public_mm/bin/metamap18 '-AIiyf --threshold 500 -R SNOMEDCT_US -J bpoc,clas,clna,cgab,dsyn,fndg,ftcn,genf,mobd,neop,ortf,phsf,qlco,qnco,spco,tmco --JSONf 2 -V USAbase'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! python2 Vocabularies/public_mm/call_metamap.py Results/concept_n_synonyms_for_post.txt.utf8 Results/concept_n_synonyms_for_post_out.txt.utf8 Results/concepts_and_synonyms_metamap_for_postCoordination_result/json Vocabularies/public_mm/bin/metamap18 '-AIiyf --threshold 500 -R SNOMEDCT_US -J bpoc,clas,clna,cgab,dsyn,fndg,ftcn,genf,mobd,neop,ortf,phsf,qlco,qnco,spco,tmco --JSONf 2 -V USAbase'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C. Read the MetaMap Result, with score over 500"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### (1) Read the result of MetaMap and filter out the terms with Score Lower than 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'Results/concepts_and_synonyms_metamap_for_postCoordination_result/'\n",
    "num_of_terms = len(os.listdir(\"Results/concepts_and_synonyms_metamap_for_postCoordination_result/\"))-1\n",
    "all_concept_list,all_CUI_list,all_score_list,all_term_list = read_metamap_result_json(file_path,num_of_terms)\n",
    "df_metamap_post = pd.DataFrame({\"Concepts\": all_concept_list, \"Matched_CUIs\": all_CUI_list, \"Matched_Scores\": all_score_list, \"Matched_Terms\": all_term_list})\n",
    "df_metamap_700 = df_metamap_post[df_metamap_post.Matched_CUIs.map(lambda d: d!=[])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### (2) Get the CUI list of all the terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is the total number of cuis: 1041\n"
     ]
    }
   ],
   "source": [
    "pre_post = df_metamap_700.copy(deep = True)\n",
    "all_cui_pre_post = []\n",
    "for i in range(len(list(pre_post.Concepts))):\n",
    "    cpt = pre_post.Concepts.tolist()[i]\n",
    "    cuis = pre_post.Matched_CUIs.tolist()[i]\n",
    "    for phase in cuis:\n",
    "        for ele in phase:\n",
    "            all_cui_pre_post.append(ele)\n",
    "    \n",
    "\n",
    "all_cui_pre_post = list(set(all_cui_pre_post))\n",
    "print(\"Here is the total number of cuis: \"+ str(len(all_cui_pre_post)))\n",
    "\n",
    "with open(\"Results/all_cui_prepost_list.txt\",'w') as output_file:\n",
    "    for key in all_cui_pre_post:\n",
    "        output_file.write(\"term\"+'\\t'+key+'\\n')\n",
    "output_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please enter your username: ilyia1997\n",
      "Please enter your password: ········\n",
      "941 runs left\n",
      "841 runs left\n",
      "741 runs left\n",
      "641 runs left\n",
      "541 runs left\n",
      "441 runs left\n",
      "341 runs left\n",
      "241 runs left\n",
      "141 runs left\n",
      "41 runs left\n"
     ]
    }
   ],
   "source": [
    "file_path = \"Results/all_cui_prepost_list.txt\"\n",
    "df_cui_snomed_prepost =  UMLS_to_SNOMED(file_path)\n",
    "df_cui_snomed_prepost.to_csv('Results/cui_snomed_prepost.txt', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cui_snomed_prepost = df_cui_snomed_prepost[df_cui_snomed_prepost.SNOMED.map(lambda d: d != '')]\n",
    "cui_snomed = dict(zip(df_cui_snomed_prepost.UMLS_CUI,df_cui_snomed_prepost.SNOMED))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### (3) Only Keep the terms that all cuis has SNOMED_ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is the number of terms that has snomed mapping: 1163\n"
     ]
    }
   ],
   "source": [
    "SNOMED_IDs_post = []\n",
    "SNOMED_Type_post = []\n",
    "SNOMED_Term_post = []\n",
    "for i in range(len(list(pre_post.Concepts))):\n",
    "    cpt = pre_post.Concepts.tolist()[i]\n",
    "    cuis = pre_post.Matched_CUIs.tolist()[i]\n",
    "    smds = []\n",
    "    types = []\n",
    "    terms = []\n",
    "    for phase in cuis:\n",
    "        smd = []\n",
    "        typ = []\n",
    "        term = []\n",
    "        for ele in phase:\n",
    "            if ele in cui_snomed:\n",
    "                smd.append(int(cui_snomed[ele]))\n",
    "                typ.append(snomedid_pharse[int(cui_snomed[ele])])\n",
    "                term.append(snomedid_term[int(cui_snomed[ele])])\n",
    "            else:\n",
    "                smd = []\n",
    "                typ = []\n",
    "                term = []\n",
    "        smds.append(smd)\n",
    "        types.append(typ)\n",
    "        terms.append(term)\n",
    "    SNOMED_IDs_post.append(smds)\n",
    "    SNOMED_Type_post.append(types)\n",
    "    SNOMED_Term_post.append(terms)\n",
    "pre_post = pre_post.reset_index()\n",
    "del pre_post['index']\n",
    "pre_post['SNOMED_ID'] = SNOMED_IDs_post\n",
    "pre_post['SNOMED_Term'] = SNOMED_Term_post\n",
    "pre_post['Semantic_Type'] = SNOMED_Type_post\n",
    "pre_post_no_snomed = pre_post[pre_post.SNOMED_ID.map(lambda d: [] in d)]\n",
    "pre_post = pre_post[pre_post.SNOMED_ID.map(lambda d: [] not in d)]\n",
    "term_snomedid = dict(zip(pre_post.Concepts,pre_post.SNOMED_ID))\n",
    "term_score = dict(zip(pre_post.Concepts,pre_post.Matched_Scores))\n",
    "snomedid_type = dict(zip(pre_post.Concepts,pre_post.Semantic_Type))\n",
    "term_term = dict(zip(pre_post.Concepts,pre_post.SNOMED_Term))\n",
    "print(\"Here is the number of terms that has snomed mapping: \"+ str(len(pre_post.SNOMED_ID.tolist())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_post_no_snomed.shape()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### (4) Prepare the Data for Post-Coordination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(664, 6)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "term_post = pre_post.Concepts.tolist()\n",
    "concept_for_post ={}\n",
    "concept_type = {}\n",
    "concept_choice = {}\n",
    "term_choice = {}\n",
    "concept_score = {}\n",
    "for term in term_post:\n",
    "    if term in dict_syn_concept:\n",
    "        concept = dict_syn_concept[term]\n",
    "    elif term in df_concept_stringmatch_fail.ORDO_term.tolist():\n",
    "        concept = term\n",
    "    else:\n",
    "        print(\"Error\")\n",
    "    max_score = 0\n",
    "    max_snomed = None\n",
    "    choice_term = None\n",
    "    sterm = None\n",
    "    if concept not in concept_for_post:\n",
    "        score = int(term_score[term][0][0][1:])\n",
    "        snomed = term_snomedid[term]\n",
    "        max_score = score\n",
    "        max_snomed = snomed\n",
    "        choice_term = term\n",
    "        sterm = term_term[term]\n",
    "        concept_for_post[concept] = max_snomed\n",
    "        concept_type[concept] = snomedid_type[term]\n",
    "        concept_choice[concept] = choice_term\n",
    "        term_choice[concept] = sterm\n",
    "        concept_score[concept] = max_score\n",
    "    else:\n",
    "        score = int(term_score[term][0][0][1:])\n",
    "        snomed = term_snomedid[term]\n",
    "        if score > max_score:\n",
    "            max_score = score\n",
    "            max_snomed = snomed\n",
    "            choice_term = term\n",
    "            sterm = term_term[term]\n",
    "            concept_for_post[concept] = max_snomed\n",
    "            concept_type[concept] = snomedid_type[term]\n",
    "            concept_choice[concept] = choice_term\n",
    "            term_choice[concept] = sterm\n",
    "            concept_score[concept] = max_score\n",
    "    \n",
    "Choice = []\n",
    "Type = []\n",
    "Term_choice = []\n",
    "STerm = []\n",
    "Mscore = []\n",
    "for concept in df_concept_stringmatch_fail.ORDO_term.tolist():\n",
    "    if concept in concept_for_post:\n",
    "        Choice.append(concept_for_post[concept])\n",
    "        Type.append(concept_type[concept])\n",
    "        Term_choice.append(concept_choice[concept])\n",
    "        STerm.append(term_choice[concept])\n",
    "        Mscore.append(concept_score[concept])\n",
    "    else:\n",
    "        Choice.append(np.nan)\n",
    "        Type.append(np.nan)\n",
    "        Term_choice.append(np.nan)\n",
    "        STerm.append(np.nan)\n",
    "        Mscore.append(np.nan)\n",
    "        \n",
    "post = pd.DataFrame({'Concepts':df_concept_stringmatch_fail.ORDO_term.tolist(),'Mapped_Score': Mscore,'Mapped_Terms':Term_choice , \"SNOMED_Terms\": STerm,\"SNOMED_ID\": Choice , 'Semantic_Type':Type})\n",
    "post = post[post.SNOMED_ID.notnull()]\n",
    "post.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### (5) Get the Post-coordination Terms only mapped to 'Disease (disorder)' and filter them out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(27, 6)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "post_remove = post[post.SNOMED_Terms.map(lambda d: d  == [['Disease (disorder)']])]\n",
    "post = post[~post.SNOMED_Terms.map(lambda d: d  == [['Disease (disorder)']])]\n",
    "post_remove.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### (6) Get the Post-coordination Terms that only mapped to one term and filter them out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(97, 6)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "domain_set={'(disorder)' ,'(morphologic abnormality)','(finding)','(observable entity)'}\n",
    "post_single = post[post.SNOMED_Terms.map(lambda d: len(d) == 1 )]\n",
    "post_single = post_single[post_single.SNOMED_Terms.map(lambda d: len(d[0]) == 1)]\n",
    "post = post[post.SNOMED_Terms.map(lambda d: len(d) != 1 or len(d[0]) != 1)]\n",
    "post_single.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(21, 6)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "post_single_keep = post_single[post_single.Mapped_Score.map(lambda d: d > 900)]\n",
    "post_single_keep = post_single_keep[post_single_keep.Semantic_Type.map(lambda d: d[0][0] in domain_set)]\n",
    "post_single_keep.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ordo_id = []\n",
    "for cpt in post_single_keep.Concepts.tolist():\n",
    "    if cpt in dict_concept_id:\n",
    "        Ordo_id.append(dict_concept_id[cpt])\n",
    "    else:\n",
    "        print(\"Error\")\n",
    "\n",
    "snomed = []\n",
    "for snomedid in post_single_keep.SNOMED_ID.tolist():\n",
    "    snomed.append(int(snomedid[0][0]))\n",
    "post_single_keep.SNOMED_ID = snomed\n",
    "snomedt = []\n",
    "for snomedid in post_single_keep.SNOMED_Terms.tolist():\n",
    "    snomedt.append(snomedid[0][0])\n",
    "post_single_keep.SNOMED_Terms = snomedt\n",
    "        \n",
    "df_stringmatch_900 = pd.DataFrame({'ORDO-ID': Ordo_id, \n",
    "                                      'Concept':post_single_keep.Concepts.tolist(),\n",
    "                                     'Mapped_Term': post_single_keep.Mapped_Terms.tolist(),\n",
    "                                     'SNOMED_ID': post_single_keep.SNOMED_ID.tolist(),\n",
    "                                     'SNOMED_Term': post_single_keep.SNOMED_Terms.tolist(),\n",
    "                                     'Score': post_single_keep.Mapped_Score.tolist()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stringmatch_900.to_csv('Results/Mapped_Results/Mapped_MetaMap900.txt',sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### (7) Do the Post-Corrdination on the remaining 67 terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_qualifier(tp):\n",
    "    idx = []\n",
    "    for i in range(len(tp)):\n",
    "        if tp[i] in qualifier_set:\n",
    "            idx.append(i)\n",
    "    return idx\n",
    "def find_site(tp):\n",
    "    idx = []\n",
    "    for i in range(len(tp)):\n",
    "        if tp[i] in findingsite_set:\n",
    "            idx.append(i)\n",
    "    return idx\n",
    "def find_attribute(tp):\n",
    "    idx = []\n",
    "    for i in range(len(tp)):\n",
    "        if tp[i] in attribute_set:\n",
    "            idx.append(i)\n",
    "    return idx\n",
    "def find_domain(tp):\n",
    "    idx = []\n",
    "    for i in range(len(tp)):\n",
    "        if tp[i] in domain_set:\n",
    "            idx.append(i)\n",
    "    return idx\n",
    "def nearest_domain(tdx, sdx):\n",
    "    st = []\n",
    "    for t in tdx:\n",
    "        d = [abs(s-t) for s in sdx]\n",
    "        min_d = min(d)\n",
    "        st.append(sdx[d.index(min_d)])\n",
    "    return st\n",
    "def concat(parts):\n",
    "    return '+'.join(parts)\n",
    "def find_post(cpt, sid, st, tp):\n",
    "    if len(find_domain(tp)) == 0:\n",
    "        return \"Not Found\"\n",
    "    \n",
    "    parts = []\n",
    "    site_index = find_site(tp)\n",
    "    qualifier_index = find_qualifier(tp)\n",
    "    attribute_index = find_attribute(tp)\n",
    "    domain_index = find_domain(tp)\n",
    "    \n",
    "    site_add_l = [\"\"]*len(tp)\n",
    "    if len(site_index) > 0:\n",
    "        site_add = nearest_domain(site_index, domain_index)\n",
    "        for i in range(len(site_add)):\n",
    "            site_add_l[site_add[i]] += \":363698007|Finding Site|=\"+str(sid[site_index[i]]) + '|' + st[site_index[i]] + '|'\n",
    "     \n",
    "    qualifier_add_l = [\"\"]*len(tp)\n",
    "    if len(qualifier_index) > 0:\n",
    "        qualifier_add = nearest_domain(qualifier_index, domain_index)\n",
    "        for i in range(len(qualifier_add)):\n",
    "            qualifier_add_l[qualifier_add[i]] += \":362981000|Qualifier Value|=\"+str(sid[qualifier_index[i]]) + '|' + st[qualifier_index[i]] + '|'\n",
    "    \n",
    "    attribute_add_l = [\"\"]*len(tp)\n",
    "    if len(attribute_index) > 0:\n",
    "        attribute_add = nearest_domain(attribute_index, domain_index)\n",
    "        for i in range(len(attribute_add)):\n",
    "            attribute_add_l[attribute_add[i]] += \":246061005|Attribute|=\"+str(sid[attribute_index[i]]) + '|' + st[attribute_index[i]] + '|'\n",
    "    \n",
    "    all_domain_terms = []\n",
    "    for i in range(len(domain_index)):\n",
    "        domain_terms = str(sid[domain_index[i]]) + '|' + st[domain_index[i]] + '|'\n",
    "        domain_terms += site_add_l[domain_index[i]]\n",
    "        domain_terms += qualifier_add_l[domain_index[i]]\n",
    "        domain_terms += attribute_add_l[domain_index[i]]\n",
    "        all_domain_terms.append(domain_terms)\n",
    "    return \"+\".join(all_domain_terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "domain_set={'(disorder)' ,'(morphologic abnormality)','(finding)','(observable entity)'}\n",
    "attribute_set = {'(attribute)': 246061005}\n",
    "qualifier_set = {'(qualifier value)': 362981000}\n",
    "findingsite_set = {'(body structure)': 363698007}\n",
    "\n",
    "post_domain = post[post.Semantic_Type.map(lambda d: domain_set.intersection(set([t for s in d for t in s])) != None)]\n",
    "\n",
    "post_domain.columns=['Concept','Score','Mapped_Terms','SNOMED_Terms','SNOMED_IDs','Semantic_Type']\n",
    "\n",
    "all_type=[]\n",
    "cpt_post={}\n",
    "for i in range(len(post_domain.SNOMED_IDs.tolist())):\n",
    "    cpt = post_domain.Concept.tolist()[i]\n",
    "    snid = post_domain.SNOMED_IDs.tolist()[i]\n",
    "    sterm = post_domain.SNOMED_Terms.tolist()[i]\n",
    "    semtype = post_domain.Semantic_Type.tolist()[i]\n",
    "    sid = []\n",
    "    st = []\n",
    "    tp = []\n",
    "    for snids in snid:\n",
    "        for ele in snids:\n",
    "            sid.append(ele)\n",
    "    for terms in sterm:\n",
    "        for term in terms:\n",
    "            st.append(term)\n",
    "    for stypes in semtype:\n",
    "        for ele in stypes:\n",
    "            tp.append(ele)\n",
    "            all_type.append(ele)\n",
    "    if cpt not in cpt_post:\n",
    "        cpt_post[cpt] = find_post(cpt, sid, st, tp)\n",
    "    else:\n",
    "        print('Error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "Post_Coordination = []\n",
    "Ordo_id = []\n",
    "for cpt in post_domain.Concept.tolist():\n",
    "    if cpt in dict_concept_id:\n",
    "        Ordo_id.append(dict_concept_id[cpt])\n",
    "    else:\n",
    "        print(\"Error\")\n",
    "    if cpt in cpt_post:\n",
    "        Post_Coordination.append(cpt_post[cpt])\n",
    "    else:\n",
    "        print(\"Error\")\n",
    "df_post_coordination = pd.DataFrame({'ORDO-ID': Ordo_id, \n",
    "                                      'Concept':post_domain.Concept.tolist(),\n",
    "                                     'Mapped_Term': post_domain.Mapped_Terms.tolist(),\n",
    "                                     'SNOMED_ID': post_domain.SNOMED_IDs.tolist(),\n",
    "                                     'SNOMED_Term': Post_Coordination,\n",
    "                                     'Score': post_domain.Score.tolist()})\n",
    "df_post_coordination = df_post_coordination[df_post_coordination.SNOMED_Term.map(lambda d: d != 'Not Found')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_post_coordination.to_csv('Results/Mapped_Results/Mapped_PostCoordiniation500.txt',sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(510, 6)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_post_coordination.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Concatenation of all the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_post_coordination.head(1)\n",
    "\n",
    "df_stringmatch_900.head(1)\n",
    "\n",
    "df_stringmatch_80.head(1)\n",
    "\n",
    "df_metamap_1000_mapped.head(1)\n",
    "\n",
    "df_Mapped_ORDO_SNOMED_Direct.head(1)\n",
    "\n",
    "df_Mapped_ORDO_SNOMED_UMLS.head(1)\n",
    "\n",
    "frames = [df_Mapped_ORDO_SNOMED_Direct,df_Mapped_ORDO_SNOMED_UMLS,df_metamap_1000_mapped,df_stringmatch_80,df_stringmatch_900,df_post_coordination]\n",
    "All_Mapped_ORDO_SNOMED = pd.concat(frames,sort=False)\n",
    "del All_Mapped_ORDO_SNOMED['Score']\n",
    "All_Mapped_ORDO_SNOMED = All_Mapped_ORDO_SNOMED.reset_index()\n",
    "del All_Mapped_ORDO_SNOMED['index']\n",
    "All_Mapped_ORDO_SNOMED.to_csv(\"Results/Mapped_Results/Mapped_ALL_ORDO_SNOMED.txt\", sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
